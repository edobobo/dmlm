_target_: src.pl_modules.TransformerDMLM
transformer_model: roberta-base
num_layers: 6
hidden_size: 512
num_heads: 8
additional_special_tokens: 2
optim_conf:
  _target_: src.utils.optimizers.RAdam
  lr: 1e-5
  weight_decay: 0.01
_recursive_: False